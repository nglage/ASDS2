{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "##### Importing relevant packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import logging #This is to provide logging of information when running the LDA\n",
    "import sys #This is to disable logging when it's no longer needed\n",
    "import pickle #To save and open previously saved machine learning models \n",
    "\n",
    "#Importing NLTK\n",
    "import nltk\n",
    "\n",
    "#Importing packages for data visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing packages for LDA\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models\n",
    "\n",
    "In this lab, we will apply a 50-topic LDA model to tweets from US Members of Congress to examine discussions about COVID-19 by Democrats and Republicans over time. Who was quicker to set the agenda around COVID-19, Democrats or Republicans? Make a guess if you know something about US politics.\n",
    "\n",
    "The dataset is the same as in the keyword discovery lab and has been preprocessed similarly. The tweets have been subsetted from 2019 and later, because we are mainly interested in tweets that (may) capture the COVID-19 crisis. The preprocessed dataset is called 'MOCTweets_preprocessed.csv.bz2'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you wish to replicate the preprocessing based on last week's full dataset, these are the steps that have been taken:\n",
    "\n",
    "1. Changing 'date' from str type object to datetime type \n",
    "2. Subsetting to 2019 and later \n",
    "3. Removing unneeded columns (all except 'date', 'text', 'affiliation', and 'nominate_name')\n",
    "2. Removing duplicated tweets \n",
    "7. Turning independents into Democrats or Republicans. In essence, finding the independents (\"SANDERS, Bernard\" and \"KING, Angus Stanley, Jr.\") and turning these to \"Democrat\". \n",
    "7. Removing instances of \"&amp\"\n",
    "8. Replacing all remaining \"&\" with \"and\"\n",
    "8. Removing odd special characters that show up in topic model (\"┻\",\"┃\",\"━\",\"┳\",\"┓\",\"┏\",\"┛\")\n",
    "9. There are some annoying cases in these data where a character that looks like a space is not a space. Here, we just replace that character with an actual space. Replacing \"\\u202F\", \"\\u2069\", \"\\u200d\", and \"\\u2066\" with \" \".\n",
    "6. Removing \"RT\" and \"via\"\n",
    "11. Removing mentions (@someone)\n",
    "10. Removing numbers, removing punctuation (except hyphens and #), removing separators, removing urls, lowercasing, removing stopwords, lemmatizing, and stemming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Preparing the data for topic modelling\n",
    "\n",
    "The preprocessed data includes columns for date, affiliation, nominate name, tweet text, and two columns of preprocessed tweet text: one stemmed and one lemmatized. We will be using the stemmed text today. \n",
    "\n",
    "1. Import the dataframe\n",
    "2. Convert 'date' from str type object to datetime type \n",
    "3. Some tweets have been fully removed - e.g., if they contained only links. Replace NaN values with an empty string in the stemmed text (NaN --> \"\"). Then use `groupby` and `agg` to group the data by *date* and *affiliation*, and aggregate the stemmed tweet text within each of these categories into one long string. The result should be one string of combined democrat tweets and one string of combined republican tweets for each date in the corpus. You do not need to save the nominate names. \n",
    "4. For each row, tokenize the stemmed text data into unigrams. Consider using NLTK's `TweetTokenizer`, which is made to handle e.g. hashtags and emojis.\n",
    "5. For each row, use `nltk.bigrams` to create bigrams from the unigrams. This function will return a list of tuples. Connect the two words in each bigram tuple with an underscore to get a list of bigram tokens. E.g.: [(great, news), (news, presid), (presid, signatur)] --> ['great_news', 'news_presid', 'presid_signatur']\n",
    "6. For each row, combine the unigrams and bigrams in one cell to create a list of all unigram and bigram tokens in that row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data\n",
    "tweets = pd.read_csv('MOCTweets_preprocessed.csv.bz2', compression = 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Creating a corpus object\n",
    "\n",
    "We will be using the `gensim` module to create a corpus object and filter very frequent and in-frequent words. Read the documentation here: https://radimrehurek.com/gensim/corpora/dictionary.html \n",
    "\n",
    "1. Create a dictionary mapping between words and their ids. This code is provided for you. This can take a couple of seconds.  \n",
    "2. Use `filter_extremes` to remove very frequent (those that appear in more than 99.9% of the documents) and very infrequent words (those that appear in less than 10 documents).\n",
    "3. You can inspect the vocabulary using either `id2token` or `token2id`. \n",
    "4. Create a corpus object. Essentially, use `doc2bow` in a list comprehension to create a list containing a bag of words for each document (aka for each row in the dataframe). Each bag of words should be a list containing tuples with 1) an index indicating the word and 2) the frequency of that word. The code could look like: `corpus = [id2word.doc2bow(doc) for doc in df['tokens']]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2972279\n"
     ]
    }
   ],
   "source": [
    "#Create a id2word dictionary\n",
    "\n",
    "#Insert the column where you saved unigram and bigram tokens between the parentheses\n",
    "id2word = Dictionary( ) \n",
    "\n",
    "#Viewing how many words are in our vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Topic modelling\n",
    "\n",
    "We will run topic modelling using an LDA implementation from the `gensim` module. You can find the documentation here: https://radimrehurek.com/gensim/models/ldamulticore.html \n",
    "\n",
    "We will investigate the results of a model with 50 topics. However, this takes several hours to run. Therefore, first try the LDA algorithm with approx. 10 topics and then download the pre-trained model to investigate the 50-topic results.\n",
    "\n",
    "1. Run the LDA algorithm with the provided code\n",
    "2. The typical way to get a sense of what each topic actually is means looking at the tokens that are most predictive for each topic. Use `print_topics` to see the 10 (or more) most predictive tokens for each topic. Optional: When using `print_topics`, the model weights are included. If you want to clean this, you can use regex to filter out everything but the words.\n",
    "\n",
    "\n",
    "**A note on setting parameters:**\n",
    "\n",
    "- Passes and iterations: As this is just a test run, passes and iterations are set quite low. If you want to use topic modelling in your project, you should make sure to set passes and iterations high enough for the model to converge. In the pre-trained model, passes = 100 and iterations = 1000. \n",
    "- Minimum_probability: This filters out all probabilities less than the given number. In the pre-trained model, minimum_probability = 0.000001\n",
    "\n",
    "\n",
    "**A note on choosing the number of topics:**\n",
    "Choosing the number of topics is not straightforward. Both Barberá (2019) and Munger (2019) use 10-fold cross-validation for the log-likelihood and perplexity. If you want to implement these models in your own work and want to learn how to do this, read about hyperparameter tuning [here](https://towardsdatascience.com/twitter-topic-modeling-e0e3315b12e2). \n",
    "\n",
    "At the same time, there is no statistic that can tell you how many topics you want: \"There is often a negative relationship between the best-fitting model and the substantive information provided\" (Grimmer and Stewart 2013). So in the end it is a judgment call on the side of the researcher, which can be guided by statistics.\n",
    "\n",
    "If topics repeat themselves, this may suggest that we chose too many topics for the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating a logger to get information on the progress of the LDA\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Running the LDA with 10 topics \n",
    "lda_model = LdaMulticore(corpus=corpus, num_topics=10, id2word=id2word, passes = 1, iterations = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disabling logging again\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Investigating results: beta parameters\n",
    "\n",
    "To investigate the results, load the pre-trained models with the provided code. \n",
    "\n",
    "Retrieve the estimated **beta parameters** from the model. There is a beta per token per topic. A beta indicates how predictive a token is of a document being assigned to a specific topic. \n",
    "\n",
    "1. Beta values can be accessed using either `get_topic_terms` (where you input the topic and number of most predictive tokens you want returned) or `get_topics` (which returns the full matrix of beta values). \n",
    "2. Pick two or three topics (in the solution code the topics 1, 8, 16, 27, 28, 29, 31, 41, 47, 48 were chosen for visualization). Get the 10 highest beta values and plot these in a horizontal barchart with `sns.barplot`, where x is the beta probability and y is the token. \n",
    "\n",
    "**Hint**: Create a dataframe from the matrix generated by `get_topics` with token names as the column names. Tokens can be retrieved with the `id2word.token2id`. Once you have the dataframe, `nlargest` can be used on a specific row to get the columns with the *n* largest values in that row. \n",
    "\n",
    "**Intended take-away**: The \"top terms\" for each topic are just the tokens that have the largest beta. \n",
    "\n",
    "**A note on the results**: You may notice that some topics have an even distributions of probabilities for all terms. This may be because the `minimum_probability` parameter is not low enough, because we do not have enough `passes` and `iterations` and the algorithm has thus not converged properly, or because we have set the number of topics too high for the data. For your own projects, experiment with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the pre-trained models\n",
    "\n",
    "lda_model=pickle.load(open('ldamodel_50t100p1000i000001minprob.sav', 'rb'))\n",
    "corpus=pickle.load(open('corpus_50t100p1000i000001minprob.sav', 'rb'))\n",
    "id2word=pickle.load(open('id2word_50t100p1000i000001minprob.sav', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Investigating results: gamma parameters\n",
    "\n",
    "Retrieve the estimated **gamma parameters** from the model. For each document, you will get a list of topics with an associated gamma. The gammas indicate the probability (or amount) that a document is about a specific topic.\n",
    "\n",
    "1. Use `get_document_topics` with the full corpus to get lists of probabilities for each document. Set `minimum_probability = 0.0`. The code will be: `lda_model.get_document_topics(corpus, minimum_probability = 0.0)`. Make this a list. \n",
    "2. Save the probabilities in the original dataframe. You can find inspiration for how to do this in the pseudo-code:\n",
    "\n",
    "**Pseudo-code to save the probabilities in the original dataframe:**\n",
    "\n",
    "Use list comprehension to create a list of 50 topic names (e.g. topic_1, topic_2 etc.)\n",
    "\n",
    "Create a dataframe filled with zeros of the shape (number of documents - 864, number of topics - 50) and with columns = the list of 50 topic names. \n",
    "\n",
    "For each index, document in the list of gamma probabilities: <br>\n",
    "<font color=white>0000</font>for each topic-probability pair in the document: <br>\n",
    "<font color=white>00000000</font>locate index and column with that topic name and save the probability\n",
    "\n",
    "Concatenate this dataframe with the original dataframe over axis 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Visualizing the development of the coronavirus related topic \n",
    "\n",
    "Getting back to the original question: How has the topic of coronavirus progressed on Twitter among Members of Congress over time? \n",
    "\n",
    "1. Use `print_topics` to look through the 10 most relevant words of each topic and identify the topic related to coronavirus. \n",
    "2. If it is not already, convert the date column from string type to datetime type\n",
    "3. Graph the development of the coronavirus topic over time. Create a plot where x = date, y = the covid topic, and hue = affiliation. \n",
    "4. Do the same with a subset of the dataset only including 2020. \n",
    "\n",
    "Can you see which party was quicker to set the agenda around COVID-19? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
