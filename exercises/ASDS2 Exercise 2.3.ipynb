{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy \n",
    "from tqdm import tqdm  #This is for creating progress bars.\n",
    "import json #To save files in json format\n",
    "import os #To set working directory\n",
    "from datetime import datetime #To check start and end time when running code\n",
    "import pickle  #To store and open previously saved machine learning models \n",
    "\n",
    "\n",
    "\n",
    "#Packages for language processing\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Packages for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Scaling\n",
    "\n",
    "Like in earlier exercise labs, we will be working with a dataset of tweets from USA Members of Congress. The version of the dataset used today includes the 3000 most recent tweets from each member, collected in 2021. We have created a subset of this dataset containing tweets from 2018 and later, to capture only fairly recent tweets as the dataset is very large. \n",
    "\n",
    "Preprocessing such a large dataset takes a while to run (approx 20 mins). To save you time and effort, we have uploaded a preprocessed version of the dataset: 'MOC2021_Tweets_2018subset_preprocessed.csv'. \n",
    "\n",
    "The following steps were applied: \n",
    "\n",
    "1. Subsetting dataset to include only tweets from 2018 and later\n",
    "1. Removing duplicated tweets \n",
    "2. Removing unneeded columns (all except 'nominate_name','affiliation','role','nominate_score', and 'text')\n",
    "3. Turning independents into Democrats or Republicans. In essence, finding the independents (\"SANDERS, Bernard\" and \"KING, Angus Stanley, Jr.\") and turning these to \"Democrat\". \n",
    "4. Replace \"&\" with \"and\"\n",
    "5. Remove odd special characters (\"┻\",\"┃\",\"━\",\"┳\",\"┓\",\"┏\",\"┛\",\"┗\")\n",
    "6. There are some annoying cases in these data where a character that looks like a space is not a space. Here, we just replace that character with an actual space. Note that the first \"space\" is not actually a space. \" \" == \" \" is FALSE. Replace \"\\u202F\", \"\\u2069\", \"\\u200d\", and \"\\u2066\" with \" \".\n",
    "7. Removing \"RT\" and \"via\"\n",
    "8. Removing mentions (@someone)\n",
    "9. Removing numbers, removing punctuation (except hyphens), removing separators, removing urls, lowercasing, removing stopwords, and lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up data paths\n",
    "\n",
    "notebook_path = os.path.realpath(os.curdir)\n",
    "\n",
    "path_to_moc_data = os.path.join(notebook_path, \"MOC2021_Tweets_2018subset_preprocessed.csv.bz2\")\n",
    "\n",
    "path_to_corp = os.path.join(notebook_path,\"Wordfish models/Wordfish models/MOCscalingresults.sav\")\n",
    "path_to_wf_scaler = os.path.join(notebook_path,\"Wordfish models/Wordfish models/MOCscaler.sav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Formatting the dataset to a shape accepted by the scaler\n",
    "\n",
    "Import the dataset with preprocessed text 'MOC2021_Tweets_2018subset_preprocessed.csv'.\n",
    "\n",
    "The dataset includes the name of each member of Congress (nominate_name), their affiliation (Democrat or Republican), their institutional role in Congress (House or Senate), their ideological score based on how they vote in Congress (nominate_score) and the text of each of their most recent 3000 tweets, reaching back until the beginning of 2018, in original and preprocessed format. \n",
    "\n",
    "1. The goal of our wordfish scaling today is to give each politician an ideological score based on their tweets. To prepare the data, we therefore need to aggregate all text per politician. In essence, transform the dataframe so that each row has one politician (rather than one tweet) and each text field includes all tweets from this politician in one long string. \n",
    "\n",
    "Hint for 1.1: When preprocessing, some tweet text was removed (e.g. if they were only URLs). To aggregate text, you may need to replace NaN values with an empty string. Next, the pandas functions `groupby` and `agg` can help you. These steps are the same as last week. \n",
    "\n",
    "\n",
    "2. The wordfish scaler accepts data in the shape of a list of tuples containing the document name and the document text. Essentially that means that you should create a list in the format: [(politician1, preprocessed_text1), (politician2, preprocessed_text2), (politician3, preprocessed_text3)].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When importing the data, use pd.read_csv('filename', compression = 'bz2')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Scaling with Wordfish\n",
    "\n",
    "The wordfish scaler we are using is from the implementation found here: https://github.com/umanlp/SemScale. Download the folder with the code from the github link by: Code --> Download zip. Alternatively, use git \n",
    "\n",
    "As this is not a (super) professional implementation, there is almost no documentation of how to use the code. Therefore, we have copied the essential parts of their `wordfish.py` code below and ask you to fill in the blanks with data from the list you have just created. \n",
    "\n",
    "You can check what the object `corp` contains with the following commands: \n",
    "- `corp.occurences`: See the document-feature matrix \n",
    "- `corp.vocubulary`: See the full vocabulary across documents. Numbers indicate their index in the dfm. \n",
    "- `corp.results`: See the scaling results \n",
    "\n",
    "The wordfish scaler takes several hours to run on the full dataset. Select a **subset of perhaps 20 politicians** to check that you have a code that works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing code \n",
    "\n",
    "Download the code from the github repository and store it somewhere on your computer. The following imports will be drawing on code in that folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting working directory to be where the SemScale code is stored \n",
    "os.chdir('./SemScale/')\n",
    "\n",
    "#Scaling packages \n",
    "#from helpers import io_helper\n",
    "from wfcode import corpus\n",
    "from wfcode import scaler\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a corpus object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a subset of your list of politicians \n",
    "\n",
    "subset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting parameters (keeping the default parameters as used in the github code)\n",
    "\n",
    "niter = 5000      #number of iterations\n",
    "lr = 0.00001      #learning rate\n",
    "stopwords = None  #we've already removed stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating a corpus object\n",
    "\n",
    "corp = corpus.Corpus() #input your data in the parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing data\n",
    "corp.tokenize(stopwords = stopwords)\n",
    "\n",
    "#Building the document-feature matrix\n",
    "corp.build_occurrences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating the corpus object\n",
    "\n",
    "Check the shape of your document-feature matrix. Check the vocabulary of the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the wordfish scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_scaler = scaler.WordfishScaler(corp)\n",
    "\n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \" WordFish scaling begun.\")\n",
    "\n",
    "wf_scaler.initialize()\n",
    "wf_scaler.train(learning_rate = lr, num_iters = niter)\n",
    "\n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \" WordFish scaling completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results \n",
    "\n",
    "View the results of the scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Validating and inspecting the results: part 1\n",
    "\n",
    "Running the wordfish scaler on the full dataset takes several hours. Therefore, we've prepared two models for you pre-trained on the full dataset. Load the models called 'MOCscalingresults.sav' (the corp object) and 'MOCscaler.sav' (the wf_scaler object) from Absalon with the code below. **Beware this is very RAM intensive** as corp is not stored as a sparse matrix. \n",
    "\n",
    "To make sure we have meaningful results, check the alpha, psi, and beta values. \n",
    "\n",
    "1. Beta values can be accessed with `wf_scaler.beta_words`. Find the 10 words that are most predictive of the low end of the ideology scale and the 10 words that are most predictive of the high end of the ideology scale. Based on these words, can you guess which ideology (Democrat vs Republican) is categorized with low values and which with high values?\n",
    "2. Alpha values can be accessed with `wf_scaler.alpha_docs`. Check the document length of the documents with the highest and lowest alpha. Do the results make sense? \n",
    "3. Psi values can be accessed with `wf_scaler.psi_words`. Check the frequency of the words with the highest and lowest psi values. Do the results make sense? \n",
    "\n",
    "Hint: All the values are stored in numpy arrays. Numpy has functions for getting the original indices of sorted values, and the index placement of minimum and maximum values, `argsort()`, `argmin()`, and `argmax()`.\n",
    "\n",
    "Also: In fact the corp document feature matrix has been initalized with a matrix of ones as a base. If you want exact results you should subtract 1 from every entry in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the saved models \n",
    "\n",
    "corp = pickle.load(open(path_to_corp, 'rb'))\n",
    "wf_scaler = pickle.load(open(path_to_wf_scaler, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Beta values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Psi values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Validating and inspecting the results: part 2\n",
    "\n",
    "1. Create a new column in your dataset and input the scaling results. \n",
    "2. To validate the results, run the overall correlation between the scaling results and the provided nominate scores. Visualize the correlation in a scatterplot. How well do the scaling results correlate with the nominate scores?\n",
    "3. Optional: Separate House and Senate. Run correlations within each institution as above and visualize as a scatterplot. \n",
    "3. Separate Democrats and Republicans. Run correlations within each party as above and visualize as a scatterplot. Can the scaling results help us determine ideological positions within each party?\n",
    "4. Find the ideological score as computed by the wordfish scaler of specific politicians that you know, e.g. Ted Cruz and Bernie Sanders. Do the results make sense? Why might the results not be as we would have expected?\n",
    "5. Based on nominate and scaling scores, respectively, who is the most extremist Republican and the most extremist Democrat? Who is the most left-wing Republican and the most right-wing Democrat? \n",
    "\n",
    "Finally, as a reflection exercise, consider the results. What are the limitations of this analysis? Could this be used in a paper or would you need to implement other methods? How could you engineer features of the text to improve the results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Merging scaling results with dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Overall correlation between our scaling results and the provided nominate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Correlations within the Senate and House, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Correlations within the two parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Scores for specific politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Most extreme Republican and Democrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 Most left-wing Republican and most right-wing Democrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
