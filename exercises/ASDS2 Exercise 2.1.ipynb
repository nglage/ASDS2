{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d620639b",
   "metadata": {
    "id": "d620639b"
   },
   "source": [
    "# Keyword expansion\n",
    "\n",
    "In this exercise we are going to use the keyword expansion technique propsoed in `Computer-Assisted Keyword and Document Set Discovery from Unstructured Text` by King, Lam and Roberts (2017), in order to label a dataset of tweets according to whether or not they are related to covid-19.\n",
    "\n",
    "The idea is to use an initial list of keywords to label the date, and then use supervised learning to expand the list of keywords to get a better sense of how people talk about a topic. It is an iterative approach, meaning that you start with a list of keywords, and expand it, run it again etc. until you saturate the list. The approach also emphasises that you should read some of the text that you label, in order to ensure correct labelling.\n",
    "\n",
    "\n",
    "This exercise is a python translation of Gregory Eady's R exercise, heavily inspired by the replication material found here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FMJDCD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf8452",
   "metadata": {
    "id": "ecdf8452"
   },
   "source": [
    "If interested, you can also see Greg's walk-through of the R version of this code in his video here:\n",
    "https://gregoryeady.com/SocialMediaDataCourse/readings/Keywords/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae27c29",
   "metadata": {
    "id": "1ae27c29"
   },
   "source": [
    "### Read in required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb6fd13b",
   "metadata": {
    "id": "bb6fd13b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import pyreadr #package to allow us to read in .rds data files (native R datafile)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from math import lgamma\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c383674",
   "metadata": {
    "id": "9c383674"
   },
   "source": [
    "# 1. Load the data\n",
    "\n",
    "Download the \"MOC-tweets\" data from the course module on Absalon, and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d322012f",
   "metadata": {
    "id": "d322012f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1615238, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>raw_url</th>\n",
       "      <th>url</th>\n",
       "      <th>url_tweet_part</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>retries</th>\n",
       "      <th>text_user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>geography</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>nominate_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190319</td>\n",
       "      <td>1.108000e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>Federal government employees are dedicated pub...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20170803</td>\n",
       "      <td>8.929066e+17</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>Congrats to @SenTomCotton's Sand Lizards on th...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>https://twitter.com/60Minutes/status/656077372...</td>\n",
       "      <td>https://twitter.com/60Minutes/status/656077372...</td>\n",
       "      <td>author</td>\n",
       "      <td>quote</td>\n",
       "      <td>20151019</td>\n",
       "      <td>6.560929e+17</td>\n",
       "      <td>0</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>WATCH: I applaud Northeast #Arkansas residents...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20181004</td>\n",
       "      <td>1.047950e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>After reviewing the FBI supplemental backgroun...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20180607</td>\n",
       "      <td>1.004722e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>Mack McLarty is a dedicated public servant who...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    user_id  num_tweets  \\\n",
       "0           1  5558312.0      430143   \n",
       "1           2  5558312.0      430143   \n",
       "2           3  5558312.0      430143   \n",
       "3           4  5558312.0      430143   \n",
       "4           5  5558312.0      430143   \n",
       "\n",
       "                                             raw_url  \\\n",
       "0                                               none   \n",
       "1                                               none   \n",
       "2  https://twitter.com/60Minutes/status/656077372...   \n",
       "3                                               none   \n",
       "4                                               none   \n",
       "\n",
       "                                                 url url_tweet_part  \\\n",
       "0                                               none           none   \n",
       "1                                               none           none   \n",
       "2  https://twitter.com/60Minutes/status/656077372...         author   \n",
       "3                                               none           none   \n",
       "4                                               none           none   \n",
       "\n",
       "  tweet_type      date      tweet_id  retries  text_user_id  \\\n",
       "0   authored  20190319  1.108000e+18   999999     5558312.0   \n",
       "1   authored  20170803  8.929066e+17   999999     5558312.0   \n",
       "2      quote  20151019  6.560929e+17        0     5558312.0   \n",
       "3   authored  20181004  1.047950e+18   999999     5558312.0   \n",
       "4   authored  20180607  1.004722e+18   999999     5558312.0   \n",
       "\n",
       "                                                text geography affiliation  \\\n",
       "0  Federal government employees are dedicated pub...        AR  Republican   \n",
       "1  Congrats to @SenTomCotton's Sand Lizards on th...        AR  Republican   \n",
       "2  WATCH: I applaud Northeast #Arkansas residents...        AR  Republican   \n",
       "3  After reviewing the FBI supplemental backgroun...        AR  Republican   \n",
       "4  Mack McLarty is a dedicated public servant who...        AR  Republican   \n",
       "\n",
       "   nominate_name  \n",
       "0  BOOZMAN, John  \n",
       "1  BOOZMAN, John  \n",
       "2  BOOZMAN, John  \n",
       "3  BOOZMAN, John  \n",
       "4  BOOZMAN, John  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/MOC_tweets.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd09dc",
   "metadata": {
    "id": "08dd09dc"
   },
   "source": [
    "# 1.1. Preprocessing\n",
    "\n",
    "Due to time restraints, the preprocessing code is given below, ready to be run. Take a look at the code to understand what is being done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5e22f",
   "metadata": {
    "id": "33a5e22f"
   },
   "source": [
    "Subset the data by removing tweets before 2019 (we are only interested in tweets that may reference COVID-19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13256c95",
   "metadata": {
    "id": "13256c95"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>raw_url</th>\n",
       "      <th>url</th>\n",
       "      <th>url_tweet_part</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>retries</th>\n",
       "      <th>text_user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>geography</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>nominate_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190319</td>\n",
       "      <td>1.108000e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>Federal government employees are dedicated pub...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>12531</td>\n",
       "      <td>https://www.vlm.cem.va.gov/?utm_source=Veteran...</td>\n",
       "      <td>https://www.vlm.cem.va.gov/?utm_source=Veteran...</td>\n",
       "      <td>author</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190913</td>\n",
       "      <td>1.172596e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>.@DeptVetAffairs recently rolled out a new dig...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>12531</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190912</td>\n",
       "      <td>1.172256e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>I know the importance of empowering women in t...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>https://www.kffb.com/us-senator-john-boozman-m...</td>\n",
       "      <td>https://www.kffb.com/us-senator-john-boozman-m...</td>\n",
       "      <td>author</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190322</td>\n",
       "      <td>1.109101e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>It was great to spend some time with leaders i...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>12531</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190918</td>\n",
       "      <td>1.174356e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>For 72 years, @usairforce has been blazing the...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    user_id  num_tweets  \\\n",
       "0            1  5558312.0      430143   \n",
       "7            8  5558312.0       12531   \n",
       "10          11  5558312.0       12531   \n",
       "17          18  5558312.0      430143   \n",
       "18          19  5558312.0       12531   \n",
       "\n",
       "                                              raw_url  \\\n",
       "0                                                none   \n",
       "7   https://www.vlm.cem.va.gov/?utm_source=Veteran...   \n",
       "10                                               none   \n",
       "17  https://www.kffb.com/us-senator-john-boozman-m...   \n",
       "18                                               none   \n",
       "\n",
       "                                                  url url_tweet_part  \\\n",
       "0                                                none           none   \n",
       "7   https://www.vlm.cem.va.gov/?utm_source=Veteran...         author   \n",
       "10                                               none           none   \n",
       "17  https://www.kffb.com/us-senator-john-boozman-m...         author   \n",
       "18                                               none           none   \n",
       "\n",
       "   tweet_type      date      tweet_id  retries  text_user_id  \\\n",
       "0    authored  20190319  1.108000e+18   999999     5558312.0   \n",
       "7    authored  20190913  1.172596e+18        0     5558312.0   \n",
       "10   authored  20190912  1.172256e+18   999999     5558312.0   \n",
       "17   authored  20190322  1.109101e+18        0     5558312.0   \n",
       "18   authored  20190918  1.174356e+18   999999     5558312.0   \n",
       "\n",
       "                                                 text geography affiliation  \\\n",
       "0   Federal government employees are dedicated pub...        AR  Republican   \n",
       "7   .@DeptVetAffairs recently rolled out a new dig...        AR  Republican   \n",
       "10  I know the importance of empowering women in t...        AR  Republican   \n",
       "17  It was great to spend some time with leaders i...        AR  Republican   \n",
       "18  For 72 years, @usairforce has been blazing the...        AR  Republican   \n",
       "\n",
       "    nominate_name  \n",
       "0   BOOZMAN, John  \n",
       "7   BOOZMAN, John  \n",
       "10  BOOZMAN, John  \n",
       "17  BOOZMAN, John  \n",
       "18  BOOZMAN, John  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.date  >= 20190101] # Subset to 2019 and later because we'll look at COVID-19 over time\n",
    "df = df.loc[df.tweet_id.drop_duplicates().index] # removing duplicate observations (tweets)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a75022b2",
   "metadata": {
    "id": "a75022b2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>raw_url</th>\n",
       "      <th>url</th>\n",
       "      <th>url_tweet_part</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>retries</th>\n",
       "      <th>text_user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>geography</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>nominate_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190319</td>\n",
       "      <td>1.108000e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>Federal government employees are dedicated pub...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>12531</td>\n",
       "      <td>https://www.vlm.cem.va.gov/?utm_source=Veteran...</td>\n",
       "      <td>https://www.vlm.cem.va.gov/?utm_source=Veteran...</td>\n",
       "      <td>author</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190913</td>\n",
       "      <td>1.172596e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>.@DeptVetAffairs recently rolled out a new dig...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>12531</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190912</td>\n",
       "      <td>1.172256e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>I know the importance of empowering women in t...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>430143</td>\n",
       "      <td>https://www.kffb.com/us-senator-john-boozman-m...</td>\n",
       "      <td>https://www.kffb.com/us-senator-john-boozman-m...</td>\n",
       "      <td>author</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190322</td>\n",
       "      <td>1.109101e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>It was great to spend some time with leaders i...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>12531</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>authored</td>\n",
       "      <td>20190918</td>\n",
       "      <td>1.174356e+18</td>\n",
       "      <td>999999</td>\n",
       "      <td>5558312.0</td>\n",
       "      <td>For 72 years, @usairforce has been blazing the...</td>\n",
       "      <td>AR</td>\n",
       "      <td>Republican</td>\n",
       "      <td>BOOZMAN, John</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    user_id  num_tweets  \\\n",
       "0           1  5558312.0      430143   \n",
       "1           8  5558312.0       12531   \n",
       "2          11  5558312.0       12531   \n",
       "3          18  5558312.0      430143   \n",
       "4          19  5558312.0       12531   \n",
       "\n",
       "                                             raw_url  \\\n",
       "0                                               none   \n",
       "1  https://www.vlm.cem.va.gov/?utm_source=Veteran...   \n",
       "2                                               none   \n",
       "3  https://www.kffb.com/us-senator-john-boozman-m...   \n",
       "4                                               none   \n",
       "\n",
       "                                                 url url_tweet_part  \\\n",
       "0                                               none           none   \n",
       "1  https://www.vlm.cem.va.gov/?utm_source=Veteran...         author   \n",
       "2                                               none           none   \n",
       "3  https://www.kffb.com/us-senator-john-boozman-m...         author   \n",
       "4                                               none           none   \n",
       "\n",
       "  tweet_type      date      tweet_id  retries  text_user_id  \\\n",
       "0   authored  20190319  1.108000e+18   999999     5558312.0   \n",
       "1   authored  20190913  1.172596e+18        0     5558312.0   \n",
       "2   authored  20190912  1.172256e+18   999999     5558312.0   \n",
       "3   authored  20190322  1.109101e+18        0     5558312.0   \n",
       "4   authored  20190918  1.174356e+18   999999     5558312.0   \n",
       "\n",
       "                                                text geography affiliation  \\\n",
       "0  Federal government employees are dedicated pub...        AR  Republican   \n",
       "1  .@DeptVetAffairs recently rolled out a new dig...        AR  Republican   \n",
       "2  I know the importance of empowering women in t...        AR  Republican   \n",
       "3  It was great to spend some time with leaders i...        AR  Republican   \n",
       "4  For 72 years, @usairforce has been blazing the...        AR  Republican   \n",
       "\n",
       "   nominate_name  \n",
       "0  BOOZMAN, John  \n",
       "1  BOOZMAN, John  \n",
       "2  BOOZMAN, John  \n",
       "3  BOOZMAN, John  \n",
       "4  BOOZMAN, John  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340dd7cd",
   "metadata": {
    "id": "340dd7cd"
   },
   "source": [
    "Save the original text and lowercase the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4550e9f5",
   "metadata": {
    "id": "4550e9f5"
   },
   "outputs": [],
   "source": [
    "df['text_original'] = df['text']\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cd89a",
   "metadata": {
    "id": "968cd89a"
   },
   "source": [
    "Do some (but not all) preprocessing by removing tweet elements that we do not care about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "943c3975",
   "metadata": {
    "id": "943c3975"
   },
   "outputs": [],
   "source": [
    "# Remove mentions (posts that start with a \"@some_user_name \")\n",
    "df['text'] = df['text'].str.replace(\"\\\\B@\\\\w+|^@\\\\w+\", \"\", regex = True)\n",
    "# Change ampersands to \"and\"\n",
    "df['text'] = df['text'].str.replace(\"&amp;\", \"and\")\n",
    "# Remove the \"RT\" and \"via\" (old retweet style)\n",
    "df['text'] = df['text'].str.replace(\"(^RT|^via)((?:\\\\b\\\\W*@\\\\w+)+)\",\"\", regex=True, case=False)\n",
    "# Remove URLs\n",
    "df['text'] = df['text'].str.replace(\"(https|http)?:\\\\/\\\\/(\\\\w|\\\\.|\\\\/|\\\\?|\\\\=|\\\\&|\\\\%)*\\\\b\", \"\", regex = True)\n",
    "# Keep ASCII only (removes Cyrillic, Japanese characters, etc.)\n",
    "df['text'] = df['text'].str.replace(\"[^ -~]\", \"\", regex = True)\n",
    "# Remove double+ spaces (e.g. \"build   the wall\" to \"build the wall\")\n",
    "df['text'] = df['text'].str.replace(\"\\\\s+\", \" \", regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e61ce",
   "metadata": {
    "id": "7d1e61ce"
   },
   "source": [
    "With our mostly preprocessed tweets, let us begin building our classifier from chosen keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837764ab",
   "metadata": {
    "id": "837764ab"
   },
   "source": [
    "# 2. Define inclusion and exclusion keywords\n",
    "\n",
    "You should now define the initial keywords that you want to include and exclude. Keywords to include should reference COVID-19, e.g. \"covid19\" and/or \"coronavirus\". We will use these initial keywords to find more keywords relevant to the topic.\n",
    "\n",
    "1. Define 4 lists: the **first** should contain a seed reference word to be included, the **second** should contain the expanded list of reference words to include (empty to begin with), the **third** should contain a seed reference word to be excluded (can be left empty), and the **fourth** should contain the expanded list of reference words to exclude (empty to begin with).\n",
    "\n",
    "2. Using `.join`, collapse the two inclusion and exclusion lists, respectively, into strings that can be used as regex OR-operations. The result should be in the form \\['dog', 'cat'\\] --> 'dog|cat'\n",
    "\n",
    "3. Use this regex string to create a bool column indicating whether the tweet contains one of your keywords.\n",
    "\n",
    "4. If you have any exlusions, also find the tweets that contain the excluded keywords (the exclusion list can be left empty).\n",
    "\n",
    "5. Define a variable that is either 0 or 1, where 1 shows that the tweet contains one or more of your inclusion keywords _and_ does not contain any exclusion keywords. Create a bool column with this.\n",
    "\n",
    "6. See how many tweets you have labelled as related to COVID-19 so far (how many 0s and how many 1s).\n",
    "\n",
    "7. Sample 10 tweets labelled as COVID-19, and read the text in them (in the text_original column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0KKeDJm0dn6N",
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1713431289668,
     "user": {
      "displayName": "Daniel Vigild",
      "userId": "08967762330835575953"
     },
     "user_tz": -120
    },
    "id": "0KKeDJm0dn6N"
   },
   "outputs": [],
   "source": [
    "# define lists for seed keywords and excluded words\n",
    "reference_words_seeds = [\"covid19\", \"coronavirus\"] # initial\n",
    "reference_words_expanded = [] # expanded\n",
    "\n",
    "reference_words_excluded = [] # initial\n",
    "reference_words_excluded_expanded = [] # expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6c3ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join lists into regex strings\n",
    "reference_words = '|'.join(reference_words_seeds + reference_words_expanded)\n",
    "reference_words_excluded = '|'.join(reference_words_excluded + reference_words_excluded_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d6d54ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19|coronavirus'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "649df9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_words = bool(reference_words_excluded.strip(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1ddb1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable to show whether inclusion keywords and no exclusion words are included\n",
    "df['included'] = df['text'].str.contains(reference_words, regex=True, case=False)\n",
    "\n",
    "df['excluded'] = False\n",
    "if excluded_words:\n",
    "    df['excluded']  = df.loc[df['text'].str.contains(reference_words_excluded, regex = True, case = False), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adfedec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reference_set'] = (df['included'] & ~df['excluded']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67d007ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference_set\n",
       "0    576881\n",
       "1      2939\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reference_set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ace34b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Though the Trump Administration’s request for emergency coronavirus funding is woefully insufficient to protect Ame… https://t.co/Caez0wk8EP\n",
      "Tweet: Like many Americans, I have questions about what the #Coronavirus means for my community.   I had the opportunity t… https://t.co/sIQ685augq\n",
      "Tweet: House Passes Bipartisan Coronavirus Response: \"Be Prepared, Not Scared,\" is still the appropriate message. To that… https://t.co/C1Pl2RG2nG\n",
      "Tweet: Completely unserious—and yet, emblematic of how most Washington Democrats are approaching the Coronavirus problem.… https://t.co/RFyEZm0NxU\n",
      "Tweet: .@barronsonline discovered that the official number of Chinese coronavirus deaths could be predicted using a simple… https://t.co/Utqjtuj57V\n",
      "Tweet: America must always be ahead of the game on viral threats like the #Coronavirus, and I’m proud to say that our gove… https://t.co/xSFDu26oWC\n",
      "Tweet: Last night we learned of the first two presumptively-positive cases of coronavirus in Florida, including one in… https://t.co/MsaTmA3GJw\n",
      "Tweet: Reduce your risk of exposure to respiratory diseases, like #COVID19: wash hands often w/ soap &amp; water for at least… https://t.co/ZDn20aUGYz\n",
      "Tweet: Update: Child under coronavirus quarantine tests negative, returns to Riverside County base https://t.co/iT8TAVlqM5\n",
      "Tweet: I spoke with @CBSEveningNews about the Coronavirus outbreak and what we can do to protect ourselves from exposure. https://t.co/27lVPw8Cos\n"
     ]
    }
   ],
   "source": [
    "# sample 10 tweets with included keywords\n",
    "sample_10 = df.loc[df['reference_set']==1,'text_original'].sample(10)\n",
    "for i in sample_10:\n",
    "    print(\"Tweet:\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb4b20",
   "metadata": {
    "id": "b6bb4b20"
   },
   "source": [
    "# 3. Further preprocessing and vectorizing\n",
    "\n",
    "Next, we need to tokenize the data and preprocess the tokens (as opposed to the preprossesing on the full string as earlier).\n",
    "\n",
    "We will also remove all the keywords that demarcate exclusion and inclusion from the covid-19 theme. This is becasue we want the model to learn to predict the topic using other, new keywords.\n",
    "\n",
    "1. Create a new col named \"text_preprocessed\" - it should be equal the text col, but with the keywords removed (Hint: use `.str.replace()` with `regex = True`).\n",
    "\n",
    "-----\n",
    "\n",
    "To spend less time on lessons you have already been through, code for further preprocessing is provided. This code may take a few minutes to run. The steps are:\n",
    "\n",
    "2. Tokenizing. A whitespace tokenizer is used, since we want to keep words with '-'.\n",
    "\n",
    "3. Removing any tokens that are only numbers (you can remove more types of tokens if you want - up to you).\n",
    "\n",
    "4. Remove any empty strings.\n",
    "\n",
    "5. Stemming.\n",
    "\n",
    "6. Re-joining the stemmed tokens using a whitespace.\n",
    "\n",
    "7. Creating a column with the preprocessed sentences.\n",
    "\n",
    "-----\n",
    "\n",
    "8. Now you have a column  of sentences made out of stemmed and preprocessed tokens. Use a CountVectorizer to make a document term matrix based on this column. Set `min_df = 10` and `max_df = 0.999`, as well as `stop_words = 'english'` and set an appropriate `ngram_range`.\n",
    "\n",
    "NB: Do not try to make this DTM into a dataframe or np array, as you will most likely run out of memory. It is a sparse matrix that you can work with in the same way as an np.array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7971f94c",
   "metadata": {
    "id": "7971f94c"
   },
   "outputs": [],
   "source": [
    "# remove keywords (inclusion & exclusion) so the model can predict the topic using other keywords\n",
    "if excluded_words:\n",
    "    all_current_keywords = reference_words + \"|\" + reference_words_excluded\n",
    "else:\n",
    "    all_current_keywords = reference_words\n",
    "\n",
    "df['text_preprossed'] = df['text'].str.replace(all_current_keywords, \"\", regex=True, case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfa51574",
   "metadata": {
    "id": "cfa51574"
   },
   "outputs": [],
   "source": [
    "tokenizer = WhitespaceTokenizer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8422774",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 369067,
     "status": "ok",
     "timestamp": 1713427439491,
     "user": {
      "displayName": "Daniel Vigild",
      "userId": "08967762330835575953"
     },
     "user_tz": -120
    },
    "id": "c8422774",
    "outputId": "f4757e86-dcfc-49d9-d912-36597128144d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/579820 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579820/579820 [05:52<00:00, 1643.61it/s]\n"
     ]
    }
   ],
   "source": [
    "pre_prossed_sents =[]\n",
    "for sent in tqdm(df['text_preprossed']): #tqdm adds progress bar\n",
    "    words = tokenizer.tokenize(sent)\n",
    "    words = [re.sub(r'\\d+', '', word) for word in words] #removing tokens that are only words\n",
    "    words = [x for x in words if x] #removing empty strings\n",
    "    sent_stem = [ps.stem(word) for word in words]\n",
    "\n",
    "    sent_done = \" \".join(sent_stem)\n",
    "    pre_prossed_sents.append(sent_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d16e846d",
   "metadata": {
    "id": "d16e846d"
   },
   "outputs": [],
   "source": [
    "df['text_pre_stem'] = pre_prossed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fa81e",
   "metadata": {
    "id": "e07fa81e"
   },
   "outputs": [],
   "source": [
    "# Consider saving a csv at this time\n",
    "df.to_csv('MOC_Tweets_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "QTYndzcXcsGU",
   "metadata": {
    "id": "QTYndzcXcsGU"
   },
   "outputs": [],
   "source": [
    "#if needed\n",
    "#df = pd.read_csv('data/MOCTweets_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9df7f064",
   "metadata": {
    "id": "9df7f064"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 10,\n",
    "            max_df = 0.999,\n",
    "            stop_words='english',\n",
    "            ngram_range = (1,2)) #set for larger n-grams\n",
    "\n",
    "corpus = vectorizer.fit_transform(df['text_pre_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1959db3",
   "metadata": {
    "id": "b1959db3"
   },
   "outputs": [],
   "source": [
    "# create document-term matrix of counts\n",
    "DTM_dict = {\"DTM\":corpus,\n",
    "               \"labels\":list(df['reference_set'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502c175",
   "metadata": {
    "id": "9502c175"
   },
   "source": [
    "# 4. Sample training data and make predictions\n",
    "\n",
    "Let us sample some tweets we will use to train our classifier.\n",
    "\n",
    "1) Define two lists of indices: One list containing the indices of the tweets in the reference set (those labelled as belonging to the covid-19 topic), and another list containing N sample of tweets not from the reference set (N should be either 2x the amount of tweets in the reference set or 50000, whichever is smaller).\n",
    "\n",
    "2) You now have 2 lists of indices – use these to subset the Document Term Matrix (where each row represents a tweet, and each column a token) and the reference set column in the dataframe (the labels). Define a train DTM and  a train labels object.\n",
    "\n",
    "3) Fit a cross validated lasso logistic regression, using the DTM subset as input (X) and the reference subset as labels (y). This means that we are trying to predict whether a tweet is in the reference set using the term frequencies. (Hint:  use the sample code with sklearn's `linear_model.LogisticRegressionCV()`). This may take some time to run (approx. 5 min, depending on the size of your train data), change some of the hyperparameters if necessary.\n",
    "\n",
    "4) Use the fitted model to make predictions on the full DTM, and create a column in the dataframe called `predicted_raw` based on this. (Remember that the rows in the DTM correspond to the rows in the dataframe).\n",
    "\n",
    "5) The prediction outputs propabilities and not classes, so check the standard deviation of the predicion_raw column - this will check if we actually have some variance in the prediction. This is just a sanity check.\n",
    "\n",
    "6) Set a threshold of 0.25, and assign 1 or 0 to a new column called `predicted`, depending on whether the probability in `predicted_raw` is >= the threshold. (Note: Keep the threshold low if you want more tweets to get into the target set).\n",
    "\n",
    "7) Create a column called `set_var`. This variable should be == \"Reference\" if the observation is in the reference set (our original covid-19 labels), \"Target\" if it is _predicted_ to be a covid-19 related tweet (1) and \"Not target\" if it is _predicted_ not to be (0).\n",
    "\n",
    "8) Create a crosstable of the prediciton and set_var, to see how you model does (hint: use use `pd.crosstab()`). Examine the crosstab - what do the different entries mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "jsGbbJpMebRq",
   "metadata": {
    "id": "jsGbbJpMebRq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5878\n"
     ]
    }
   ],
   "source": [
    "# Determine how many tweets to sample for the training data\n",
    "n_search = min(sum(df['reference_set']==1)* 2, 50000)\n",
    "print(n_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa35ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5878\n"
     ]
    }
   ],
   "source": [
    "# list containing indices of tweets in reference set\n",
    "ref_ids = df[df['reference_set']==1].index.tolist()\n",
    "\n",
    "# N sample of tweets not from reference set (N should be either 2x the amont of tweets in the reference set of 50000, whichever is smaller)\n",
    "n_search = min(sum(df['reference_set']==1)* 2, 50000)\n",
    "print(n_search)\n",
    "\n",
    "search_ids = df[df['reference_set']==0].index.tolist()\n",
    "\n",
    "search_ids_sample = random.sample(search_ids, n_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db2ccada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2939 5878 8817\n"
     ]
    }
   ],
   "source": [
    "# subset dtm and labels to defain train dtm and train labels object\n",
    "ids = ref_ids + search_ids_sample\n",
    "\n",
    "print(len(ref_ids),len(search_ids_sample),len(ids))\n",
    "\n",
    "X_train = corpus[ids]\n",
    "y_train = df.loc[ids, 'reference_set']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e5d16dcb",
   "metadata": {
    "id": "e5d16dcb"
   },
   "outputs": [],
   "source": [
    "# Defining the classifier\n",
    "\n",
    "#logistic reg with lasso penalty\n",
    "clf = linear_model.LogisticRegressionCV(\n",
    "    penalty=\"l1\", \n",
    "    n_jobs = -1, \n",
    "    solver = \"saga\", \n",
    "    max_iter=10000, \n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2c61d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "executionInfo": {
     "elapsed": 13375,
     "status": "ok",
     "timestamp": 1713431299544,
     "user": {
      "displayName": "Daniel Vigild",
      "userId": "08967762330835575953"
     },
     "user_tz": -120
    },
    "id": "2cc2c61d",
    "outputId": "48ae2334-994a-45f2-f1ff-3d5299acc557",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 356 epochs took 20 seconds\n",
      "convergence after 32 epochs took 1 seconds\n",
      "convergence after 425 epochs took 22 seconds\n",
      "convergence after 444 epochs took 23 seconds\n",
      "convergence after 372 epochs took 23 seconds\n",
      "convergence after 29 epochs took 1 seconds\n",
      "convergence after 30 epochs took 1 seconds\n",
      "convergence after 29 epochs took 1 seconds\n",
      "convergence after 437 epochs took 24 seconds\n",
      "convergence after 31 epochs took 2 seconds\n",
      "convergence after 214 epochs took 9 seconds\n",
      "convergence after 306 epochs took 10 seconds\n",
      "convergence after 319 epochs took 11 seconds\n",
      "convergence after 521 epochs took 18 seconds\n",
      "convergence after 561 epochs took 19 seconds\n",
      "convergence after 1197 epochs took 727 seconds\n",
      "convergence after 1961 epochs took 626 seconds\n",
      "convergence after 2356 epochs took 642 seconds\n",
      "convergence after 2527 epochs took 661 seconds\n",
      "convergence after 2587 epochs took 651 seconds\n",
      "convergence after 2624 epochs took 666 seconds\n",
      "convergence after 1256 epochs took 735 seconds\n",
      "convergence after 1745 epochs took 788 seconds\n",
      "convergence after 1869 epochs took 794 seconds\n",
      "convergence after 1988 epochs took 828 seconds\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model. This takes some time.\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c857ec6",
   "metadata": {
    "id": "1c857ec6"
   },
   "source": [
    "# 5. Calculate the log likelihood as in the paper\n",
    "\n",
    "1) Create 3 sets of indices based on the `set_var` colum: one for \"Target\", one for \"Not target\" and one for \"Reference\".\n",
    "\n",
    "2) Create 3 objects for the target, not_target and reference sets, based on the DTM. These should be: for each token, how often is the given token in the set, how many documents in the set contains the given token, and the proportion of documents in the set containing the given token. (Hint: see sample code for the target set. If you want to convert to a list and not a matrix object, you can use the `.tolist()[0]`)\n",
    "\n",
    "3) Create a new dataframe, where each row is a token from the DTM (you can use `vectorizer.get_feature_names()`), with 9 cols for each of the 9 objects you just created.\n",
    "\n",
    "4) Subset the dataset by removing any observations where the terms do not appear in either the target or not_target set, thus keeping only tokens that were in the original search set (step (a) on page 979).\n",
    "\n",
    "5) Keywords go in the target list if their proportion is higher among those documents estimated to be in the reference set than not; e.g. if for the word \"pandemic\", 15% of documents predicted as target contain the word \"pandemic\" versus only 2% among those in the not_target set (step (b) on page 979). Therefore: create a new column that should be True if the token has a higher or equal proportion in the target set than in the not_target set.\n",
    "\n",
    "6) Examine the `llik` function provided and look in the paper - what does it do?\n",
    "\n",
    "7) Calculate the amount of documents in the target and the not_target set.\n",
    "\n",
    "8) Use the provided function to calculate the log likelihood for each token. Assign this to a new column in the dataframe created in step 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158bef3c",
   "metadata": {
    "id": "158bef3c"
   },
   "outputs": [],
   "source": [
    "# Creating 3 lists of indices\n",
    "\n",
    "target_ids = list(pd.Series(DTM_full['set_var']) == 'Target')\n",
    "not_target_ids =\n",
    "ref_ids ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61811aae",
   "metadata": {
    "id": "61811aae"
   },
   "outputs": [],
   "source": [
    "# Creating statistics for the target, not_target and reference sets\n",
    "\n",
    "target_freq = np.sum(DTM_full['DTM'][target_ids,:],0) #how many times is each token used in the target documents\n",
    "target_num_docs = np.sum(DTM_full['DTM'][target_ids,:] > 0, axis = 0) #how many target documents does each token appear in\n",
    "target_num_docs_prop =  target_num_docs / sum(target_ids) #proportion of target docs with each token\n",
    "\n",
    "\n",
    "not_target_freq =\n",
    "not_target_num_docs =\n",
    "not_target_num_docs_prop =\n",
    "\n",
    "\n",
    "ref_freq =\n",
    "ref_num_docs =\n",
    "ref_num_docs_prop =\n",
    "\n",
    "\n",
    "# Saving the above in a dict\n",
    "\n",
    "d = {'target_freq' :target_freq.tolist()[0],\n",
    "    'target_num_docs': target_num_docs.tolist()[0],\n",
    "    'target_num_docs_prop': target_num_docs_prop.tolist()[0],\n",
    "    'not_target_freq':\n",
    "    'not_target_num_docs':\n",
    "    'not_target_num_docs_prop':\n",
    "    'ref_freq':\n",
    "    'ref_num_docs':\n",
    "    'ref_num_docs_prop': }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005dcab",
   "metadata": {
    "id": "c005dcab"
   },
   "outputs": [],
   "source": [
    "# Likelihood function\n",
    "\n",
    "def llik(target_num_docs, nottarget_num_docs, target_num_docs_total, nottarget_num_docs_total):\n",
    "    '''No docstring - you neew to see what it does :) '''\n",
    "    x1 = ((lgamma(target_num_docs + 1) + lgamma(nottarget_num_docs + 1)) -\n",
    "           lgamma(target_num_docs + nottarget_num_docs + 1 + 1))\n",
    "    x2 = ((lgamma(target_num_docs_total - target_num_docs + 1) +\n",
    "           lgamma(nottarget_num_docs_total - nottarget_num_docs + 1)) -\n",
    "           lgamma(target_num_docs_total - target_num_docs +\n",
    "          nottarget_num_docs_total - nottarget_num_docs + 1 + 1))\n",
    "    llik = x1 + x2\n",
    "    return llik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ed22d",
   "metadata": {
    "id": "293ed22d"
   },
   "source": [
    "# 6. Examine new keywords\n",
    "\n",
    "1) Show the top 25 keywords based on highest log likelihood, where the share of documents in the target set is higher than in the not_target set (see task 5.5). These are the tokens that are most likely to differentiate between the target and not_target sets (meaning that they help the model predict covid-19 related tweets).\n",
    "\n",
    "2) Do the same with the not_target - what are these terms representative of?\n",
    "\n",
    "3) Are there any of these tokens that you want to include in the keywords? Choose 1-3 keywords that you want to include or exclude.\n",
    "\n",
    "4) For the 1-3 keywords you have found, find tweets that contain the given keyword in the original tweet text in the original dataframe. Read some tweets where the keyword is used in context - do you still want to include or exclude the keyword?\n",
    "\n",
    "5) Optional: add the new keywords to the original list at the beginning of this exercise in 2.1, and rerun the exercises until here, now including the new keywords. This is how the computer-assisted keyword discovery is used iteratively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1PFEmuBue5eB",
   "metadata": {
    "id": "1PFEmuBue5eB"
   },
   "outputs": [],
   "source": [
    "# [your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb9bb3",
   "metadata": {
    "id": "2ebb9bb3"
   },
   "source": [
    "# 7. Optional: Use your new classifier for downstream tasks\n",
    "\n",
    "1) Assign a `final_classification` boolean column in the original dataframe, which should be 1 if the tweet contains any of the keywords in the new, complete list and if it does not contain any of the exclusion keywords.\n",
    "\n",
    "2) Examine the value counts of the political affiliation variable. Assign \"Democrat\" to the tweets labelled with \"Independent\" (see the people behind the tweets for reason).\n",
    "\n",
    "3) Plot the share of tweets labelled as covid-19 relevant by your classifier (y), grouped on days (x) for each party - meaning two lines of covid-19 share across time.\n",
    "\n",
    "**Hints:** <br>\n",
    "The pandas `groupby` functionality may be of help to you. <br>\n",
    "You can also also turn the date ints into so-called datetime objects using this:\n",
    "\n",
    "`dates =[datetime.datetime(year=int(x[i][0:4]), month=int(x[i][4:6]), day=int(x[i][6:8])) for i in range(len(x))]`\n",
    "\n",
    "where x is a list of the unique dates as int.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gCzk_1xtW5Al",
   "metadata": {
    "id": "gCzk_1xtW5Al"
   },
   "outputs": [],
   "source": [
    "# [ your code here]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
